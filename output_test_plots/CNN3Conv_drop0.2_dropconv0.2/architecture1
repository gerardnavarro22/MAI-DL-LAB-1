digraph {
	graph [size="15.75,15.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140297113255616 [label="
 (128, 29)" fillcolor=darkolivegreen1]
	140297113417088 [label=AddmmBackward0]
	140297113410656 -> 140297113417088
	140297113506480 [label="fc2.bias
 (29)" fillcolor=lightblue]
	140297113506480 -> 140297113410656
	140297113410656 [label=AccumulateGrad]
	140297113410752 -> 140297113417088
	140297113410752 [label=NativeDropoutBackward0]
	140297113416272 -> 140297113410752
	140297113416272 [label=ReluBackward0]
	140297113404224 -> 140297113416272
	140297113404224 [label=MiopenBatchNormBackward0]
	140297113405808 -> 140297113404224
	140297113405808 [label=AddmmBackward0]
	140297113406720 -> 140297113405808
	140297113515280 [label="fc1.bias
 (512)" fillcolor=lightblue]
	140297113515280 -> 140297113406720
	140297113406720 [label=AccumulateGrad]
	140297113417520 -> 140297113405808
	140297113417520 [label=ViewBackward0]
	140297113406576 -> 140297113417520
	140297113406576 [label=NativeDropoutBackward0]
	140297113414304 -> 140297113406576
	140297113414304 [label=MaxPool2DWithIndicesBackward0]
	140297113417136 -> 140297113414304
	140297113417136 [label=ReluBackward0]
	140297113408688 -> 140297113417136
	140297113408688 [label=ConvolutionBackward0]
	140297113414880 -> 140297113408688
	140297113414880 [label=MaxPool2DWithIndicesBackward0]
	140297113413584 -> 140297113414880
	140297113413584 [label=ReluBackward0]
	140297113410176 -> 140297113413584
	140297113410176 [label=MiopenBatchNormBackward0]
	140297113409168 -> 140297113410176
	140297113409168 [label=ConvolutionBackward0]
	140297113418384 -> 140297113409168
	140297113418384 [label=MaxPool2DWithIndicesBackward0]
	140297113419440 -> 140297113418384
	140297113419440 [label=ReluBackward0]
	140297113410224 -> 140297113419440
	140297113410224 [label=MiopenBatchNormBackward0]
	140297113410608 -> 140297113410224
	140297113410608 [label=ConvolutionBackward0]
	140297113404032 -> 140297113410608
	140297305754576 [label="conv1.weight
 (32, 3, 5, 5)" fillcolor=lightblue]
	140297305754576 -> 140297113404032
	140297113404032 [label=AccumulateGrad]
	140297113407344 -> 140297113410608
	140297312167120 [label="conv1.bias
 (32)" fillcolor=lightblue]
	140297312167120 -> 140297113407344
	140297113407344 [label=AccumulateGrad]
	140297113418096 -> 140297113410224
	140297193023072 [label="batch_norm_conv_1.weight
 (32)" fillcolor=lightblue]
	140297193023072 -> 140297113418096
	140297113418096 [label=AccumulateGrad]
	140297113407632 -> 140297113410224
	140297113063152 [label="batch_norm_conv_1.bias
 (32)" fillcolor=lightblue]
	140297113063152 -> 140297113407632
	140297113407632 [label=AccumulateGrad]
	140297113417040 -> 140297113409168
	140297295350656 [label="conv2.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	140297295350656 -> 140297113417040
	140297113417040 [label=AccumulateGrad]
	140297113418192 -> 140297113409168
	140297312759984 [label="conv2.bias
 (64)" fillcolor=lightblue]
	140297312759984 -> 140297113418192
	140297113418192 [label=AccumulateGrad]
	140297113415360 -> 140297113410176
	140297113512160 [label="batch_norm_conv_2.weight
 (64)" fillcolor=lightblue]
	140297113512160 -> 140297113415360
	140297113415360 [label=AccumulateGrad]
	140297113407200 -> 140297113410176
	140297113517120 [label="batch_norm_conv_2.bias
 (64)" fillcolor=lightblue]
	140297113517120 -> 140297113407200
	140297113407200 [label=AccumulateGrad]
	140297113409648 -> 140297113408688
	140297113508560 [label="conv3.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140297113508560 -> 140297113409648
	140297113409648 [label=AccumulateGrad]
	140297113406288 -> 140297113408688
	140297113517200 [label="conv3.bias
 (128)" fillcolor=lightblue]
	140297113517200 -> 140297113406288
	140297113406288 [label=AccumulateGrad]
	140297113408832 -> 140297113405808
	140297113408832 [label=TBackward0]
	140297113411328 -> 140297113408832
	140297113504960 [label="fc1.weight
 (512, 131072)" fillcolor=lightblue]
	140297113504960 -> 140297113411328
	140297113411328 [label=AccumulateGrad]
	140297113409936 -> 140297113404224
	140297113511760 [label="batch_norm_fcnn.weight
 (512)" fillcolor=lightblue]
	140297113511760 -> 140297113409936
	140297113409936 [label=AccumulateGrad]
	140297113416608 -> 140297113404224
	140297113502960 [label="batch_norm_fcnn.bias
 (512)" fillcolor=lightblue]
	140297113502960 -> 140297113416608
	140297113416608 [label=AccumulateGrad]
	140297113417760 -> 140297113417088
	140297113417760 [label=TBackward0]
	140297113404320 -> 140297113417760
	140297113501760 [label="fc2.weight
 (29, 512)" fillcolor=lightblue]
	140297113501760 -> 140297113404320
	140297113404320 [label=AccumulateGrad]
	140297113417088 -> 140297113255616
}
