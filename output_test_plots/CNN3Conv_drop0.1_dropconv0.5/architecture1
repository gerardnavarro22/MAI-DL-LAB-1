digraph {
	graph [size="15.75,15.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140297113250416 [label="
 (128, 29)" fillcolor=darkolivegreen1]
	140297112847600 [label=AddmmBackward0]
	140297112856624 -> 140297112847600
	140297113248176 [label="fc2.bias
 (29)" fillcolor=lightblue]
	140297113248176 -> 140297112856624
	140297112856624 [label=AccumulateGrad]
	140297112862576 -> 140297112847600
	140297112862576 [label=NativeDropoutBackward0]
	140297112848416 -> 140297112862576
	140297112848416 [label=ReluBackward0]
	140297112858112 -> 140297112848416
	140297112858112 [label=MiopenBatchNormBackward0]
	140297112861328 -> 140297112858112
	140297112861328 [label=AddmmBackward0]
	140297112857632 -> 140297112861328
	140297113245456 [label="fc1.bias
 (512)" fillcolor=lightblue]
	140297113245456 -> 140297112857632
	140297112857632 [label=AccumulateGrad]
	140297112857392 -> 140297112861328
	140297112857392 [label=ViewBackward0]
	140297112852736 -> 140297112857392
	140297112852736 [label=NativeDropoutBackward0]
	140297112858352 -> 140297112852736
	140297112858352 [label=MaxPool2DWithIndicesBackward0]
	140297112852976 -> 140297112858352
	140297112852976 [label=ReluBackward0]
	140297112855568 -> 140297112852976
	140297112855568 [label=ConvolutionBackward0]
	140297112860848 -> 140297112855568
	140297112860848 [label=MaxPool2DWithIndicesBackward0]
	140297112857248 -> 140297112860848
	140297112857248 [label=ReluBackward0]
	140297113417808 -> 140297112857248
	140297113417808 [label=MiopenBatchNormBackward0]
	140297113417280 -> 140297113417808
	140297113417280 [label=ConvolutionBackward0]
	140297113414784 -> 140297113417280
	140297113414784 [label=MaxPool2DWithIndicesBackward0]
	140297113406336 -> 140297113414784
	140297113406336 [label=ReluBackward0]
	140297113405136 -> 140297113406336
	140297113405136 [label=MiopenBatchNormBackward0]
	140297113404416 -> 140297113405136
	140297113404416 [label=ConvolutionBackward0]
	140297113405712 -> 140297113404416
	140297308116992 [label="conv1.weight
 (32, 3, 5, 5)" fillcolor=lightblue]
	140297308116992 -> 140297113405712
	140297113405712 [label=AccumulateGrad]
	140297113405568 -> 140297113404416
	140297312167120 [label="conv1.bias
 (32)" fillcolor=lightblue]
	140297312167120 -> 140297113405568
	140297113405568 [label=AccumulateGrad]
	140297113403600 -> 140297113405136
	140297193023312 [label="batch_norm_conv_1.weight
 (32)" fillcolor=lightblue]
	140297193023312 -> 140297113403600
	140297113403600 [label=AccumulateGrad]
	140297113405472 -> 140297113405136
	140297193023072 [label="batch_norm_conv_1.bias
 (32)" fillcolor=lightblue]
	140297193023072 -> 140297113405472
	140297113405472 [label=AccumulateGrad]
	140297113416656 -> 140297113417280
	140297113203184 [label="conv2.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	140297113203184 -> 140297113416656
	140297113416656 [label=AccumulateGrad]
	140297113417328 -> 140297113417280
	140297113194384 [label="conv2.bias
 (64)" fillcolor=lightblue]
	140297113194384 -> 140297113417328
	140297113417328 [label=AccumulateGrad]
	140297113416416 -> 140297113417808
	140297113203104 [label="batch_norm_conv_2.weight
 (64)" fillcolor=lightblue]
	140297113203104 -> 140297113416416
	140297113416416 [label=AccumulateGrad]
	140297113414928 -> 140297113417808
	140297113202784 [label="batch_norm_conv_2.bias
 (64)" fillcolor=lightblue]
	140297113202784 -> 140297113414928
	140297113414928 [label=AccumulateGrad]
	140297112862624 -> 140297112855568
	140297113249136 [label="conv3.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140297113249136 -> 140297112862624
	140297112862624 [label=AccumulateGrad]
	140297112857104 -> 140297112855568
	140297113252176 [label="conv3.bias
 (128)" fillcolor=lightblue]
	140297113252176 -> 140297112857104
	140297112857104 [label=AccumulateGrad]
	140297112852208 -> 140297112861328
	140297112852208 [label=TBackward0]
	140297112858880 -> 140297112852208
	140297113248016 [label="fc1.weight
 (512, 131072)" fillcolor=lightblue]
	140297113248016 -> 140297112858880
	140297112858880 [label=AccumulateGrad]
	140297112851536 -> 140297112858112
	140297113251136 [label="batch_norm_fcnn.weight
 (512)" fillcolor=lightblue]
	140297113251136 -> 140297112851536
	140297112851536 [label=AccumulateGrad]
	140297112860944 -> 140297112858112
	140297113251536 [label="batch_norm_fcnn.bias
 (512)" fillcolor=lightblue]
	140297113251536 -> 140297112860944
	140297112860944 [label=AccumulateGrad]
	140297112860224 -> 140297112847600
	140297112860224 [label=TBackward0]
	140297112851056 -> 140297112860224
	140297113253936 [label="fc2.weight
 (29, 512)" fillcolor=lightblue]
	140297113253936 -> 140297112851056
	140297112851056 [label=AccumulateGrad]
	140297112847600 -> 140297113250416
}
