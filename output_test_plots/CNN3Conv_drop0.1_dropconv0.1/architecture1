digraph {
	graph [size="15.75,15.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140297113254896 [label="
 (128, 29)" fillcolor=darkolivegreen1]
	140297113267280 [label=AddmmBackward0]
	140297113267136 -> 140297113267280
	140297113249296 [label="fc2.bias
 (29)" fillcolor=lightblue]
	140297113249296 -> 140297113267136
	140297113267136 [label=AccumulateGrad]
	140297113269536 -> 140297113267280
	140297113269536 [label=NativeDropoutBackward0]
	140297113258784 -> 140297113269536
	140297113258784 [label=ReluBackward0]
	140297113268048 -> 140297113258784
	140297113268048 [label=MiopenBatchNormBackward0]
	140297113267328 -> 140297113268048
	140297113267328 [label=AddmmBackward0]
	140297113257536 -> 140297113267328
	140297113247056 [label="fc1.bias
 (512)" fillcolor=lightblue]
	140297113247056 -> 140297113257536
	140297113257536 [label=AccumulateGrad]
	140297113271312 -> 140297113267328
	140297113271312 [label=ViewBackward0]
	140297113257680 -> 140297113271312
	140297113257680 [label=NativeDropoutBackward0]
	140297113268480 -> 140297113257680
	140297113268480 [label=MaxPool2DWithIndicesBackward0]
	140297113268864 -> 140297113268480
	140297113268864 [label=ReluBackward0]
	140297113263296 -> 140297113268864
	140297113263296 [label=ConvolutionBackward0]
	140297113262480 -> 140297113263296
	140297113262480 [label=MaxPool2DWithIndicesBackward0]
	140297113264448 -> 140297113262480
	140297113264448 [label=ReluBackward0]
	140297113259216 -> 140297113264448
	140297113259216 [label=MiopenBatchNormBackward0]
	140297113256288 -> 140297113259216
	140297113256288 [label=ConvolutionBackward0]
	140297113264016 -> 140297113256288
	140297113264016 [label=MaxPool2DWithIndicesBackward0]
	140297113267808 -> 140297113264016
	140297113267808 [label=ReluBackward0]
	140297113259120 -> 140297113267808
	140297113259120 [label=MiopenBatchNormBackward0]
	140297113260656 -> 140297113259120
	140297113260656 [label=ConvolutionBackward0]
	140297113265456 -> 140297113260656
	140297113203824 [label="conv1.weight
 (32, 3, 5, 5)" fillcolor=lightblue]
	140297113203824 -> 140297113265456
	140297113265456 [label=AccumulateGrad]
	140297113267184 -> 140297113260656
	140297311113424 [label="conv1.bias
 (32)" fillcolor=lightblue]
	140297311113424 -> 140297113267184
	140297113267184 [label=AccumulateGrad]
	140297113265696 -> 140297113259120
	140297286798368 [label="batch_norm_conv_1.weight
 (32)" fillcolor=lightblue]
	140297286798368 -> 140297113265696
	140297113265696 [label=AccumulateGrad]
	140297113256144 -> 140297113259120
	140297112943360 [label="batch_norm_conv_1.bias
 (32)" fillcolor=lightblue]
	140297112943360 -> 140297113256144
	140297113256144 [label=AccumulateGrad]
	140297113257488 -> 140297113256288
	140297113067312 [label="conv2.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	140297113067312 -> 140297113257488
	140297113257488 [label=AccumulateGrad]
	140297113257344 -> 140297113256288
	140297113074992 [label="conv2.bias
 (64)" fillcolor=lightblue]
	140297113074992 -> 140297113257344
	140297113257344 [label=AccumulateGrad]
	140297113258112 -> 140297113259216
	140297113062112 [label="batch_norm_conv_2.weight
 (64)" fillcolor=lightblue]
	140297113062112 -> 140297113258112
	140297113258112 [label=AccumulateGrad]
	140297113268576 -> 140297113259216
	140297113063072 [label="batch_norm_conv_2.bias
 (64)" fillcolor=lightblue]
	140297113063072 -> 140297113268576
	140297113268576 [label=AccumulateGrad]
	140297113258208 -> 140297113263296
	140297113249536 [label="conv3.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140297113249536 -> 140297113258208
	140297113258208 [label=AccumulateGrad]
	140297113259360 -> 140297113263296
	140297113251216 [label="conv3.bias
 (128)" fillcolor=lightblue]
	140297113251216 -> 140297113259360
	140297113259360 [label=AccumulateGrad]
	140297113272032 -> 140297113267328
	140297113272032 [label=TBackward0]
	140297113270976 -> 140297113272032
	140297113249616 [label="fc1.weight
 (512, 131072)" fillcolor=lightblue]
	140297113249616 -> 140297113270976
	140297113270976 [label=AccumulateGrad]
	140297113264976 -> 140297113268048
	140297113250896 [label="batch_norm_fcnn.weight
 (512)" fillcolor=lightblue]
	140297113250896 -> 140297113264976
	140297113264976 [label=AccumulateGrad]
	140297113269872 -> 140297113268048
	140297113248576 [label="batch_norm_fcnn.bias
 (512)" fillcolor=lightblue]
	140297113248576 -> 140297113269872
	140297113269872 [label=AccumulateGrad]
	140297113269392 -> 140297113267280
	140297113269392 [label=TBackward0]
	140297113271984 -> 140297113269392
	140297113254336 [label="fc2.weight
 (29, 512)" fillcolor=lightblue]
	140297113254336 -> 140297113271984
	140297113271984 [label=AccumulateGrad]
	140297113267280 -> 140297113254896
}
