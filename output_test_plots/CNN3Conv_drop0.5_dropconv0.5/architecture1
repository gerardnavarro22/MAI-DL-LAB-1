digraph {
	graph [size="15.75,15.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140297113249136 [label="
 (128, 29)" fillcolor=darkolivegreen1]
	140297113267712 [label=AddmmBackward0]
	140297113259360 -> 140297113267712
	140297113502880 [label="fc2.bias
 (29)" fillcolor=lightblue]
	140297113502880 -> 140297113259360
	140297113259360 [label=AccumulateGrad]
	140297113266512 -> 140297113267712
	140297113266512 [label=NativeDropoutBackward0]
	140297113256768 -> 140297113266512
	140297113256768 [label=ReluBackward0]
	140297113268864 -> 140297113256768
	140297113268864 [label=MiopenBatchNormBackward0]
	140297113261520 -> 140297113268864
	140297113261520 [label=AddmmBackward0]
	140297113266944 -> 140297113261520
	140297113510400 [label="fc1.bias
 (512)" fillcolor=lightblue]
	140297113510400 -> 140297113266944
	140297113266944 [label=AccumulateGrad]
	140297113270976 -> 140297113261520
	140297113270976 [label=ViewBackward0]
	140297113264976 -> 140297113270976
	140297113264976 [label=NativeDropoutBackward0]
	140297113262528 -> 140297113264976
	140297113262528 [label=MaxPool2DWithIndicesBackward0]
	140297113268432 -> 140297113262528
	140297113268432 [label=ReluBackward0]
	140297113267904 -> 140297113268432
	140297113267904 [label=ConvolutionBackward0]
	140297113258160 -> 140297113267904
	140297113258160 [label=MaxPool2DWithIndicesBackward0]
	140297113257200 -> 140297113258160
	140297113257200 [label=ReluBackward0]
	140297113270112 -> 140297113257200
	140297113270112 [label=MiopenBatchNormBackward0]
	140297113261184 -> 140297113270112
	140297113261184 [label=ConvolutionBackward0]
	140297113269920 -> 140297113261184
	140297113269920 [label=MaxPool2DWithIndicesBackward0]
	140297113256144 -> 140297113269920
	140297113256144 [label=ReluBackward0]
	140297113266032 -> 140297113256144
	140297113266032 [label=MiopenBatchNormBackward0]
	140297113263920 -> 140297113266032
	140297113263920 [label=ConvolutionBackward0]
	140297113259168 -> 140297113263920
	140297112943360 [label="conv1.weight
 (32, 3, 5, 5)" fillcolor=lightblue]
	140297112943360 -> 140297113259168
	140297113259168 [label=AccumulateGrad]
	140297113258208 -> 140297113263920
	140297112943040 [label="conv1.bias
 (32)" fillcolor=lightblue]
	140297112943040 -> 140297113258208
	140297113258208 [label=AccumulateGrad]
	140297113271984 -> 140297113266032
	140297300485328 [label="batch_norm_conv_1.weight
 (32)" fillcolor=lightblue]
	140297300485328 -> 140297113271984
	140297113271984 [label=AccumulateGrad]
	140297113267232 -> 140297113266032
	140297312751664 [label="batch_norm_conv_1.bias
 (32)" fillcolor=lightblue]
	140297312751664 -> 140297113267232
	140297113267232 [label=AccumulateGrad]
	140297113264784 -> 140297113261184
	140297297433744 [label="conv2.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	140297297433744 -> 140297113264784
	140297113264784 [label=AccumulateGrad]
	140297113262384 -> 140297113261184
	140297189483808 [label="conv2.bias
 (64)" fillcolor=lightblue]
	140297189483808 -> 140297113262384
	140297113262384 [label=AccumulateGrad]
	140297113266704 -> 140297113270112
	140297113075152 [label="batch_norm_conv_2.weight
 (64)" fillcolor=lightblue]
	140297113075152 -> 140297113266704
	140297113266704 [label=AccumulateGrad]
	140297113263632 -> 140297113270112
	140297113062192 [label="batch_norm_conv_2.bias
 (64)" fillcolor=lightblue]
	140297113062192 -> 140297113263632
	140297113263632 [label=AccumulateGrad]
	140297113258112 -> 140297113267904
	140297295353216 [label="conv3.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140297295353216 -> 140297113258112
	140297113258112 [label=AccumulateGrad]
	140297113264880 -> 140297113267904
	140297257581792 [label="conv3.bias
 (128)" fillcolor=lightblue]
	140297257581792 -> 140297113264880
	140297113264880 [label=AccumulateGrad]
	140297113270544 -> 140297113261520
	140297113270544 [label=TBackward0]
	140297113269728 -> 140297113270544
	140297113518000 [label="fc1.weight
 (512, 131072)" fillcolor=lightblue]
	140297113518000 -> 140297113269728
	140297113269728 [label=AccumulateGrad]
	140297113265024 -> 140297113268864
	140297113506480 [label="batch_norm_fcnn.weight
 (512)" fillcolor=lightblue]
	140297113506480 -> 140297113265024
	140297113265024 [label=AccumulateGrad]
	140297113268336 -> 140297113268864
	140297113513360 [label="batch_norm_fcnn.bias
 (512)" fillcolor=lightblue]
	140297113513360 -> 140297113268336
	140297113268336 [label=AccumulateGrad]
	140297113265456 -> 140297113267712
	140297113265456 [label=TBackward0]
	140297113270448 -> 140297113265456
	140297113517600 [label="fc2.weight
 (29, 512)" fillcolor=lightblue]
	140297113517600 -> 140297113270448
	140297113270448 [label=AccumulateGrad]
	140297113267712 -> 140297113249136
}
