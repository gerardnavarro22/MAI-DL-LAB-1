digraph {
	graph [size="15.75,15.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140297113505360 [label="
 (128, 29)" fillcolor=darkolivegreen1]
	140297112860416 [label=AddmmBackward0]
	140297112862096 -> 140297112860416
	140297113247136 [label="fc2.bias
 (29)" fillcolor=lightblue]
	140297113247136 -> 140297112862096
	140297112862096 [label=AccumulateGrad]
	140297112861136 -> 140297112860416
	140297112861136 [label=NativeDropoutBackward0]
	140297112859264 -> 140297112861136
	140297112859264 [label=ReluBackward0]
	140297112853744 -> 140297112859264
	140297112853744 [label=MiopenBatchNormBackward0]
	140297112858544 -> 140297112853744
	140297112858544 [label=AddmmBackward0]
	140297112849136 -> 140297112858544
	140297113251376 [label="fc1.bias
 (512)" fillcolor=lightblue]
	140297113251376 -> 140297112849136
	140297112849136 [label=AccumulateGrad]
	140297112854176 -> 140297112858544
	140297112854176 [label=ViewBackward0]
	140297112856768 -> 140297112854176
	140297112856768 [label=NativeDropoutBackward0]
	140297112861088 -> 140297112856768
	140297112861088 [label=MaxPool2DWithIndicesBackward0]
	140297112853648 -> 140297112861088
	140297112853648 [label=ReluBackward0]
	140297112856624 -> 140297112853648
	140297112856624 [label=ConvolutionBackward0]
	140297112858880 -> 140297112856624
	140297112858880 [label=MaxPool2DWithIndicesBackward0]
	140297112855280 -> 140297112858880
	140297112855280 [label=ReluBackward0]
	140297112861184 -> 140297112855280
	140297112861184 [label=MiopenBatchNormBackward0]
	140297112854224 -> 140297112861184
	140297112854224 [label=ConvolutionBackward0]
	140297112860992 -> 140297112854224
	140297112860992 [label=MaxPool2DWithIndicesBackward0]
	140297112855472 -> 140297112860992
	140297112855472 [label=ReluBackward0]
	140297112850624 -> 140297112855472
	140297112850624 [label=MiopenBatchNormBackward0]
	140297112860368 -> 140297112850624
	140297112860368 [label=ConvolutionBackward0]
	140297112854416 -> 140297112860368
	140297286663952 [label="conv1.weight
 (32, 3, 5, 5)" fillcolor=lightblue]
	140297286663952 -> 140297112854416
	140297112854416 [label=AccumulateGrad]
	140297112857392 -> 140297112860368
	140297257581152 [label="conv1.bias
 (32)" fillcolor=lightblue]
	140297257581152 -> 140297112857392
	140297112857392 [label=AccumulateGrad]
	140297112855856 -> 140297112850624
	140297113202704 [label="batch_norm_conv_1.weight
 (32)" fillcolor=lightblue]
	140297113202704 -> 140297112855856
	140297112855856 [label=AccumulateGrad]
	140297112862288 -> 140297112850624
	140297113194304 [label="batch_norm_conv_1.bias
 (32)" fillcolor=lightblue]
	140297113194304 -> 140297112862288
	140297112862288 [label=AccumulateGrad]
	140297112857152 -> 140297112854224
	140297112776720 [label="conv2.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	140297112776720 -> 140297112857152
	140297112857152 [label=AccumulateGrad]
	140297112846544 -> 140297112854224
	140297113253456 [label="conv2.bias
 (64)" fillcolor=lightblue]
	140297113253456 -> 140297112846544
	140297112846544 [label=AccumulateGrad]
	140297112854368 -> 140297112861184
	140297113255856 [label="batch_norm_conv_2.weight
 (64)" fillcolor=lightblue]
	140297113255856 -> 140297112854368
	140297112854368 [label=AccumulateGrad]
	140297112854464 -> 140297112861184
	140297113253616 [label="batch_norm_conv_2.bias
 (64)" fillcolor=lightblue]
	140297113253616 -> 140297112854464
	140297112854464 [label=AccumulateGrad]
	140297112855664 -> 140297112856624
	140297113255696 [label="conv3.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140297113255696 -> 140297112855664
	140297112855664 [label=AccumulateGrad]
	140297112852928 -> 140297112856624
	140297113254576 [label="conv3.bias
 (128)" fillcolor=lightblue]
	140297113254576 -> 140297112852928
	140297112852928 [label=AccumulateGrad]
	140297112857680 -> 140297112858544
	140297112857680 [label=TBackward0]
	140297112861664 -> 140297112857680
	140297113244736 [label="fc1.weight
 (512, 131072)" fillcolor=lightblue]
	140297113244736 -> 140297112861664
	140297112861664 [label=AccumulateGrad]
	140297112852880 -> 140297112853744
	140297113246896 [label="batch_norm_fcnn.weight
 (512)" fillcolor=lightblue]
	140297113246896 -> 140297112852880
	140297112852880 [label=AccumulateGrad]
	140297112854560 -> 140297112853744
	140297113254976 [label="batch_norm_fcnn.bias
 (512)" fillcolor=lightblue]
	140297113254976 -> 140297112854560
	140297112854560 [label=AccumulateGrad]
	140297112857632 -> 140297112860416
	140297112857632 [label=TBackward0]
	140297112860272 -> 140297112857632
	140297113251216 [label="fc2.weight
 (29, 512)" fillcolor=lightblue]
	140297113251216 -> 140297112860272
	140297112860272 [label=AccumulateGrad]
	140297112860416 -> 140297113505360
}
