digraph {
	graph [size="15.75,15.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140297113253296 [label="
 (128, 29)" fillcolor=darkolivegreen1]
	140297113272032 [label=AddmmBackward0]
	140297113261184 -> 140297113272032
	140297113511280 [label="fc2.bias
 (29)" fillcolor=lightblue]
	140297113511280 -> 140297113261184
	140297113261184 [label=AccumulateGrad]
	140297113257728 -> 140297113272032
	140297113257728 [label=NativeDropoutBackward0]
	140297113267712 -> 140297113257728
	140297113267712 [label=ReluBackward0]
	140297113259552 -> 140297113267712
	140297113259552 [label=MiopenBatchNormBackward0]
	140297113260176 -> 140297113259552
	140297113260176 [label=AddmmBackward0]
	140297113266800 -> 140297113260176
	140297112941840 [label="fc1.bias
 (512)" fillcolor=lightblue]
	140297112941840 -> 140297113266800
	140297113266800 [label=AccumulateGrad]
	140297113260992 -> 140297113260176
	140297113260992 [label=ViewBackward0]
	140297113269680 -> 140297113260992
	140297113269680 [label=NativeDropoutBackward0]
	140297113269920 -> 140297113269680
	140297113269920 [label=MaxPool2DWithIndicesBackward0]
	140297113258400 -> 140297113269920
	140297113258400 [label=ReluBackward0]
	140297113265456 -> 140297113258400
	140297113265456 [label=ConvolutionBackward0]
	140297113266512 -> 140297113265456
	140297113266512 [label=MaxPool2DWithIndicesBackward0]
	140297113256480 -> 140297113266512
	140297113256480 [label=ReluBackward0]
	140297113257488 -> 140297113256480
	140297113257488 [label=MiopenBatchNormBackward0]
	140297113262384 -> 140297113257488
	140297113262384 [label=ConvolutionBackward0]
	140297113266032 -> 140297113262384
	140297113266032 [label=MaxPool2DWithIndicesBackward0]
	140297113256336 -> 140297113266032
	140297113256336 [label=ReluBackward0]
	140297113260512 -> 140297113256336
	140297113260512 [label=MiopenBatchNormBackward0]
	140297113267856 -> 140297113260512
	140297113267856 [label=ConvolutionBackward0]
	140297113266368 -> 140297113267856
	140297312167120 [label="conv1.weight
 (32, 3, 5, 5)" fillcolor=lightblue]
	140297312167120 -> 140297113266368
	140297113266368 [label=AccumulateGrad]
	140297113259360 -> 140297113267856
	140297187547296 [label="conv1.bias
 (32)" fillcolor=lightblue]
	140297187547296 -> 140297113259360
	140297113259360 [label=AccumulateGrad]
	140297113271984 -> 140297113260512
	140297295350656 [label="batch_norm_conv_1.weight
 (32)" fillcolor=lightblue]
	140297295350656 -> 140297113271984
	140297113271984 [label=AccumulateGrad]
	140297113263824 -> 140297113260512
	140297193023312 [label="batch_norm_conv_1.bias
 (32)" fillcolor=lightblue]
	140297193023312 -> 140297113263824
	140297113263824 [label=AccumulateGrad]
	140297113272176 -> 140297113262384
	140297113067312 [label="conv2.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	140297113067312 -> 140297113272176
	140297113272176 [label=AccumulateGrad]
	140297113269632 -> 140297113262384
	140297113063152 [label="conv2.bias
 (64)" fillcolor=lightblue]
	140297113063152 -> 140297113269632
	140297113269632 [label=AccumulateGrad]
	140297113265024 -> 140297113257488
	140297113062112 [label="batch_norm_conv_2.weight
 (64)" fillcolor=lightblue]
	140297113062112 -> 140297113265024
	140297113265024 [label=AccumulateGrad]
	140297113269296 -> 140297113257488
	140297113202784 [label="batch_norm_conv_2.bias
 (64)" fillcolor=lightblue]
	140297113202784 -> 140297113269296
	140297113269296 [label=AccumulateGrad]
	140297113267424 -> 140297113265456
	140297113203824 [label="conv3.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140297113203824 -> 140297113267424
	140297113267424 [label=AccumulateGrad]
	140297113257104 -> 140297113265456
	140297113194144 [label="conv3.bias
 (128)" fillcolor=lightblue]
	140297113194144 -> 140297113257104
	140297113257104 [label=AccumulateGrad]
	140297113261136 -> 140297113260176
	140297113261136 [label=TBackward0]
	140297113267232 -> 140297113261136
	140297112941600 [label="fc1.weight
 (512, 131072)" fillcolor=lightblue]
	140297112941600 -> 140297113267232
	140297113267232 [label=AccumulateGrad]
	140297113269584 -> 140297113259552
	140297113075312 [label="batch_norm_fcnn.weight
 (512)" fillcolor=lightblue]
	140297113075312 -> 140297113269584
	140297113269584 [label=AccumulateGrad]
	140297113271024 -> 140297113259552
	140297112942160 [label="batch_norm_fcnn.bias
 (512)" fillcolor=lightblue]
	140297112942160 -> 140297113271024
	140297113271024 [label=AccumulateGrad]
	140297113258160 -> 140297113272032
	140297113258160 [label=TBackward0]
	140297113263344 -> 140297113258160
	140297113513680 [label="fc2.weight
 (29, 512)" fillcolor=lightblue]
	140297113513680 -> 140297113263344
	140297113263344 [label=AccumulateGrad]
	140297113272032 -> 140297113253296
}
