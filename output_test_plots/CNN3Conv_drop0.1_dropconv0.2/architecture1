digraph {
	graph [size="15.75,15.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140297113252576 [label="
 (128, 29)" fillcolor=darkolivegreen1]
	140297113405952 [label=AddmmBackward0]
	140297113416800 -> 140297113405952
	140297113253296 [label="fc2.bias
 (29)" fillcolor=lightblue]
	140297113253296 -> 140297113416800
	140297113416800 [label=AccumulateGrad]
	140297113417424 -> 140297113405952
	140297113417424 [label=NativeDropoutBackward0]
	140297113403888 -> 140297113417424
	140297113403888 [label=ReluBackward0]
	140297113418432 -> 140297113403888
	140297113418432 [label=MiopenBatchNormBackward0]
	140297113418528 -> 140297113418432
	140297113418528 [label=AddmmBackward0]
	140297113403936 -> 140297113418528
	140297113061952 [label="fc1.bias
 (512)" fillcolor=lightblue]
	140297113061952 -> 140297113403936
	140297113403936 [label=AccumulateGrad]
	140297113404992 -> 140297113418528
	140297113404992 [label=ViewBackward0]
	140297113419392 -> 140297113404992
	140297113419392 [label=NativeDropoutBackward0]
	140297113406288 -> 140297113419392
	140297113406288 [label=MaxPool2DWithIndicesBackward0]
	140297113403792 -> 140297113406288
	140297113403792 [label=ReluBackward0]
	140297113406864 -> 140297113403792
	140297113406864 [label=ConvolutionBackward0]
	140297113411040 -> 140297113406864
	140297113411040 [label=MaxPool2DWithIndicesBackward0]
	140297113404080 -> 140297113411040
	140297113404080 [label=ReluBackward0]
	140297113404944 -> 140297113404080
	140297113404944 [label=MiopenBatchNormBackward0]
	140297113412288 -> 140297113404944
	140297113412288 [label=ConvolutionBackward0]
	140297113417040 -> 140297113412288
	140297113417040 [label=MaxPool2DWithIndicesBackward0]
	140297113405664 -> 140297113417040
	140297113405664 [label=ReluBackward0]
	140297113404032 -> 140297113405664
	140297113404032 [label=MiopenBatchNormBackward0]
	140297113417376 -> 140297113404032
	140297113417376 [label=ConvolutionBackward0]
	140297113403648 -> 140297113417376
	140297303570800 [label="conv1.weight
 (32, 3, 5, 5)" fillcolor=lightblue]
	140297303570800 -> 140297113403648
	140297113403648 [label=AccumulateGrad]
	140297113268384 -> 140297113417376
	140297311431440 [label="conv1.bias
 (32)" fillcolor=lightblue]
	140297311431440 -> 140297113268384
	140297113268384 [label=AccumulateGrad]
	140297113419488 -> 140297113404032
	140297113062192 [label="batch_norm_conv_1.weight
 (32)" fillcolor=lightblue]
	140297113062192 -> 140297113419488
	140297113419488 [label=AccumulateGrad]
	140297113404656 -> 140297113404032
	140297113061792 [label="batch_norm_conv_1.bias
 (32)" fillcolor=lightblue]
	140297113061792 -> 140297113404656
	140297113404656 [label=AccumulateGrad]
	140297113406192 -> 140297113412288
	140297113244816 [label="conv2.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	140297113244816 -> 140297113406192
	140297113406192 [label=AccumulateGrad]
	140297113405376 -> 140297113412288
	140297113251936 [label="conv2.bias
 (64)" fillcolor=lightblue]
	140297113251936 -> 140297113405376
	140297113405376 [label=AccumulateGrad]
	140297113417664 -> 140297113404944
	140297113255456 [label="batch_norm_conv_2.weight
 (64)" fillcolor=lightblue]
	140297113255456 -> 140297113417664
	140297113417664 [label=AccumulateGrad]
	140297113405616 -> 140297113404944
	140297113247936 [label="batch_norm_conv_2.bias
 (64)" fillcolor=lightblue]
	140297113247936 -> 140297113405616
	140297113405616 [label=AccumulateGrad]
	140297113405232 -> 140297113406864
	140297113247136 [label="conv3.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140297113247136 -> 140297113405232
	140297113405232 [label=AccumulateGrad]
	140297113415936 -> 140297113406864
	140297113244656 [label="conv3.bias
 (128)" fillcolor=lightblue]
	140297113244656 -> 140297113415936
	140297113415936 [label=AccumulateGrad]
	140297113418864 -> 140297113418528
	140297113418864 [label=TBackward0]
	140297113410224 -> 140297113418864
	140297113061712 [label="fc1.weight
 (512, 131072)" fillcolor=lightblue]
	140297113061712 -> 140297113410224
	140297113410224 [label=AccumulateGrad]
	140297113418384 -> 140297113418432
	140297113245776 [label="batch_norm_fcnn.weight
 (512)" fillcolor=lightblue]
	140297113245776 -> 140297113418384
	140297113418384 [label=AccumulateGrad]
	140297113418048 -> 140297113418432
	140297113252656 [label="batch_norm_fcnn.bias
 (512)" fillcolor=lightblue]
	140297113252656 -> 140297113418048
	140297113418048 [label=AccumulateGrad]
	140297113417520 -> 140297113405952
	140297113417520 [label=TBackward0]
	140297113419248 -> 140297113417520
	140297113250736 [label="fc2.weight
 (29, 512)" fillcolor=lightblue]
	140297113250736 -> 140297113419248
	140297113419248 [label=AccumulateGrad]
	140297113405952 -> 140297113252576
}
